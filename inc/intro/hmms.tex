\label{sec:hmms}
Proteins, RNAs and other biological molecular sequences can usually be
classified into families of related sequences and structures
(\cite{henikoff1997}). Since a similarity measure bears no genealogical meaning,
as explained in \autoref{sec:orthology-howto}, more appropriate approaches must
be tried.

Hidden Markov models (HMMs) are statistical models that are generally applicable
to ``linear'' problems. Because of this property, HMMs have seen widespread use
in temporal pattern recognition algorithms, such as speech and gesture
recognition or musical score following for over thirty years \citep{rabiner1989}
before they were introduced into computational biology by \citet{churchill1989}
and used as profile models since the 1990s \citep{krogh1994}. Their statistical,
linear nature makes them very useful for application on nucleic or amino acid
sequences, which are usually linear and can be modelled in such a fashion. 

The basis of a HMM is a stochastic process called Markov chain, which was named
after the mathematician Andrey Markov who described such processes in the late
19th century. A Markov chain is a sequence of states $s_{i1}, s_{i2}, ...
s_{ik}, ...$ that is generated by a process that moves with a certain probability
from one state to the next. The probability of each subsequent state depends
only on the previous state:

\begin{equation}
P(s_{ik} | s_{i1}, s_{i2}, ..., s_{ik-1}) = P(s_{ik} | s_{ik-1})
\label{eqn:markov-chain}
\end{equation}

For example, in a simple Markov model with the two possible states ``rain''
($s_1$) and ``dry'' ($s_2$) that have the transition probabilities $P(s_1|s_1) =
0.3$, $P(s_2|s_2) = 0.2$, $P(s_2|s_1) = 0.7$, $P(s_1|s_2) = 0.8$, and the
initial probabilities $P(s_1) = 0.4$, $P(s_2) = 0.6$, the probability of the
state sequence ``dry'', ``dry'', ``rain'', ``rain'' is

\begin{equation}
P(\{s_2, s_2, s_1, s_1\}) = P(s_1|s_1) P(s_1|s_2) P(s_2|s_2) P(s_2) = 0.3 \cdot 0.2 \cdot 0.8 \cdot 0.6
\label{eqn:markov-chain-weather}
\end{equation}

or, written more generally

\begin{equation}
	\begin{split}
		P(s) &= P(s_L|s_{L-1}) P(s_{L-1}|s_{L-2}) . . . P(s_2) P(s_1) \\
		&= P(s_1) \prod^{L}_{i=2}a_{s_{i-1}s_{i}}
	\end{split}
	\label{eqn:markov-chain-general}
\end{equation}

where $a$ is the transition probability from one given state to the next and $L$
is the length of the sequence.

In a \emph{hidden Markov model}, there are multiple chains that are invisible to the
observer. A simple HMM consists of a two-state chain that emits a sequence of
characters, e.g., nucleotide symbols, each with a probability that is dependent
on the state. The state chain transits between two states that have different
emission probabilities for the four possible symbols. Only the output sequence
is visible to the observer, the state sequence remains hidden
(\autoref{fig:hmm}). More generally, a sequence is generated by a HMM as
follows: First a state $\pi_1$ is chosen according to the probabilities $a_{0i}$.
Then a new state $\pi_2$ is chosen according to the transition probabilities
$a_{\pi_{1}i}$ and so forth \citep{durbin1998}. The general joint probability of
an observed sequence $x$ and a state sequence $\pi$ is written as follows:

\begin{equation}
P(x,\pi) = a_{0\pi_1} \prod_{i=1}^L e_{\pi_i}(x_i)a_{\pi_i\pi_{i+1}}
\label{eqn:hmm-general}
\end{equation}

where $\pi_{L+1} = 0$ is required. Equation \eqref{eqn:hmm-general} is the HMM
analogue of equation \eqref{eqn:markov-chain-general} \citep{durbin1998}.

\input{inc/intro/fig-hmm-eddy}

HMMs can be used in a pairwise sequence alignment algorithm using transition
scores according to the transition probabilities. The transitions are assigned a
score increment, and the states each specify a $\Delta(i,i)$ pair. The alignment
algorithm uses a \emph{finite state automaton} (FSA), which is a concept from
computer science and describes an abstract machine that can be in one of a
finite number of states. A FSA is defined by a list of its states and the
triggering condition for each transition, and can be described with a HMM. An
alignment corresponds to a path through the states, with symbols from the
underlying pair of sequences being transferred to the alignment according to the
$\Delta(i,j)$ values in the states \citep{durbin1998}.
