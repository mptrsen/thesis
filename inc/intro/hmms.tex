\label{sec:hmms}
Proteins, RNAs and other biological molecular sequences can usually be
classified into families of related sequences and structures
(\cite{henikoff1997}). Since a similarity measure bears no genealogical meaning,
as explained in \autoref{sec:orthology-howto}, more appropriate approaches must
be tried.

Hidden Markov models (HMMs) are statistical models that are generally applicable
to ``linear'' problems. Because of this property, HMMs have seen widespread use
in temporal pattern recognition algorithms, such as speech and gesture
recognition or musical score following for over thirty years before they were
introduced into computational biology by \citet{churchill1989} and used as
profile models since the 1990s \citep{krogh1994}. Their statistical, linear
nature makes them very useful for application on nucleic or amino acid
sequences, which are usually linear and can be modelled in such a fashion. 

The basis of a HMM is a stochastic process called Markov chain, which was named
after the mathematician Andrey Markov who described such processes in the late
19th century. A Markov chain is a sequence of states $s_{i1}, s_{i2}, ...
s_{ik}, ...$ that is generated by a process that moves with a certain probability
from one state to the next. The probability of each subsequent state depends
only on the previous state:

\begin{equation}
P(s_{ik} | s_{i1}, s_{i2}, ..., s_{ik-1}) = P(s_{ik} | s_{ik-1})
\label{eqn:markov-chain}
\end{equation}

For example, in a simple Markov model with the two possible states ``rain''
($s_1$) and ``dry'' ($s_2$) that have the transition probabilities $P(s_1|s_1) =
0.3$, $P(s_2|s_1) = 0.7$, $P(s_1|s_2) = 0.8$, and the initial probabilities
$P(s_1) = 0.4$, $P(s_2) = 0.6$, the probability of the state sequence ``dry'',
``dry'', ``rain'', ``rain'' is

\begin{equation}
P(\{s_2, s_2, s_1, s_1\}) = P(s_1|s_1) P(s_1|s_2) P(s_2|s_2) P(s_2) = 0.3 \cdot 0.2 \cdot 0.8 \cdot 0.6
\label{eqn:markov-chain-weather}
\end{equation}

In a \emph{hidden Markov model}, there are multiple chains that are invisible to the
observer. A simple HMM consists of a two-state chain that emits a sequence of
characters, e.g., nucleotide symbols, each with a probability that is dependent
on the state. The state chain transits between two states that have different
emission probabilities for the four possible symbols. Only the output sequence
is visible to the observer, the state sequence remains hidden
(\autoref{fig:hmm}).

\input{inc/intro/fig-hmm-eddy}

\todo{elaborate?}

