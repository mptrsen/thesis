\label{sec:hmms}
Biological molecular sequences such as amino acid and nucleotide sequences can
usually be classified into families that share homologous features, \eg, a
similar three-dimensional structure or a particular sequence of amino acids
\citep{henikoff1997}. Since a similarity measure bears no genealogical
meaning---as explained in \autoref{sec:orthology-howto}---more appropriate
approaches must be employed to identify homology.

Hidden Markov models (HMMs) are statistical models that are particularly well
suited to process so-called ``linear'' problems. Because of this property, HMMs
have seen widespread use in temporal pattern recognition algorithms, such as
speech and gesture recognition or musical score following for over thirty years
\citep{rabiner1989}. They were introduced into computational biology by
\citet{churchill1989} and used as profile models since the 1990s
\citep{krogh1994}. Their statistical, linear nature makes them very useful for
application on nucleic or amino acid sequences, which are usually linear and can
be modelled in such a fashion. 

The basis of a HMM is a stochastic process called Markov chain, which was named
after the mathematician Andrey Markov who described them in the late 19th
century. A Markov chain is a sequence of states $s_{i1}, s_{i2}, ...  s_{ik},
...$ that is generated by a process that transits with a certain probability
from one state to the next. The probability of each subsequent state depends
only on the previous state:

\begin{equation}
P(s_{ik} | s_{i1}, s_{i2}, ..., s_{ik-1}) = P(s_{ik} | s_{ik-1})
\label{eqn:markov-chain}
\end{equation}

For example, in a simple Markov chain with the two possible states ``rain''
($s_1$) and ``dry'' ($s_2$) that have the transition probabilities $P(s_1|s_1) =
0.3$, $P(s_2|s_2) = 0.2$, $P(s_2|s_1) = 0.7$, $P(s_1|s_2) = 0.8$, and the
initial probabilities $P(s_1) = 0.4$, $P(s_2) = 0.6$, the probability of the
state sequence ``dry'', ``dry'', ``rain'', ``rain'' is

\begin{equation}
	\begin{split}
		P(\{s_2, s_2, s_1, s_1\}) &= P(s_2) P(s_2|s_2) P(s_1|s_2) P(s_1|s_1) \\
		&= 0.6 \cdot 0.8 \cdot 0.2 \cdot 0.3 
	\end{split}
\label{eqn:markov-chain-weather}
\end{equation}

or, written more generally

\begin{equation}
	\begin{split}
		P(s) &= P(s_L|s_{L-1}) P(s_{L-1}|s_{L-2}) . . . P(s_2) P(s_1) \\
		&= P(s_1) \prod^{L}_{i=2}a_{s_{i-1}s_{i}}
	\end{split}
	\label{eqn:markov-chain-general}
\end{equation}

where $a$ is the transition probability from one given state to the next and $L$
is the length of the sequence.

In a \emph{hidden Markov model} (see \autoref{fig:hmm} on page \pageref{fig:hmm}), there are multiple chains that are invisible to
the observer, and only its output can be interpreted. A simple HMM consists of a
two-state Markov chain that \emph{emits} a sequence of characters, \eg, nucleotide
symbols, each with a probability that is dependent on the state of the chain. The state chain
transits between two states that have different emission probabilities for the
four possible symbols. Only the output sequence is visible to the observer, the
state sequence remains hidden.
% However, using stochastic theory, it can be inferred.
Generally, a HMM generates a state sequence as follows: First a state $s_1$ is
chosen according to the probabilities $a_{0i}$. Then a new state $s_2$ is
chosen according to the transition probabilities $a_{s_{1}i}$ and so forth
\citep{durbin1998}. The general joint probability of an observed sequence $x$
and a state sequence $s$ is written as follows:

\begin{equation}
P(x,s) = a_{0s_1} \prod_{i=1}^L e_{s_i}(x_i)a_{s_is_{i+1}}
\label{eqn:hmm-general}
\end{equation}

where $a_{s_{L+1}} = 0$ is required. Equation \eqref{eqn:hmm-general} is the HMM
analogue of equation \eqref{eqn:markov-chain-general} \citep{durbin1998}.

\input{inc/intro/fig-hmm-eddy}

HMMs can be used in a pairwise sequence alignment algorithm using transition
scores: the transitions are assigned a score increment according to the
transition probabilities, and the resulting states each specify a $\Delta(i,j)$
pair for the transition from $s_i$ to $s_j$. The alignment algorithm uses a
\emph{finite state automaton} (FSA), which is a concept from computer science
and describes an abstract machine that can be in one of a finite number of
states. A FSA is defined by a list of its states and the triggering condition
for each transition, and can be described with a HMM. An alignment corresponds
to a path through the states, with symbols from the underlying pair of
sequences (which can also be gaps) being transferred to the alignment according
to the $\Delta(i,j)$ values in the states \citep{durbin1998}. 

\todo{elaborate}

HMM-based alignment algorithms have been demonstrated to be very effective in
detecting conserved patterns in multiple amino acid sequences \citep{eddy1995,
hughey1996}.  This performance is achieved by \emph{training} the HMM on a
multiple sequence alignment of amino acid sequences (when attempting to align
amino acid sequences) that are members of a protein family. The resulting HMM
can discriminate between protein family members and non-family members with
great accuracy, even if they are very remotely related to each other
\citep{karplus1998}. In the HMMER3 implementation (see \autoref{sec:programs}),
HMM searches exhibit a better specificity---that is, a better ratio of
$\textrm{[true positives/true positives+false negatives]}$
\citep{korf2004}---than both BLAST \citep{altschul1990} and PSI-BLAST
\citep{altschul1997} (benchmark tests by \citet{eddy2009}).

